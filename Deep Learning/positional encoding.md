Складываем позиционные энкодинги с векторами слов.

## [[Transformers]]
In context of [[Attention is All You Need! (2017)]] Paper, and [[Transformers]] in general, Positional Encodings can be noticed on the inputs/ outputs of a network.
![[Pasted image 20240119155235.png|300x400]]



## How it works?
![[Pasted image 20231130112207.png]]

![[Pasted image 20231130112223.png]]
