- RNN is too long
- Backwards pass goes through time
- Transformer - NN for [[Seq2Seq]] based on [[Fully-connected Neural Networks|Fully-connected Neural Layers]]
- Is better than [[Seq2Seq]] architectures for quality and speed.
- Main element - Multi-Head Self-Attention

![[Pasted image 20231130110016.png]]

## Last Levels
![[Pasted image 20231130110047.png]]

## What happens in encoder?
![[Pasted image 20231130112323.png]]

![[Pasted image 20231130112342.png]]


## High Level
Transformer - is just encoder and decoder.