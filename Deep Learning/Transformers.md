![[Pasted image 20231130112700.png]]

## How do they work?
Input for Encoders block and for Decoders block is always [[Embeddings]]

Model consists of 6 Encoders and 6 Decoders.
![[Pasted image 20231130110016.png|w90]]

## Single Encoder
![[Pasted image 20240119154033.png|w75]]
All words are fed at the same time during training. 

![[Positional Encoding]]

Positionally encoded input and output embeddings are then fed into Encoder and Decoder. 

## ![[Multi-Head-Attention]]


Transformers use [[Self-Supervised Learning]].
- [[SBERT]]
- [[BERT]]
- [[GPT]]

## Encoder
![[Pasted image 20231207131054.png]]





Related:
[[Attention Mechanism]]
[[Attention is All You Need! (2017)]]