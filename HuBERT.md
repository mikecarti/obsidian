![[Pasted image 20240118220633.png]]

HuBERT - модель для создания *представлений* (Representations) голоса. Это как раз видимо причина ключевого слова *Retrieval* в методе, которым мы пользуемся (Retrieval-Based Voice Conversion). Главная фича: модель учится расставлять фразы И по лингвистическим соображениям И по аудио-соображениям. 

То есть, чем лучше unsupervised кластеринг прошел по аудиозаписям, тем лучше будут представления. 

Как они достигают такой фичи? То есть заставляют модель быть мульти-модальной в каком-то плане: 
Они нагладывают маску [MSK] на некоторые выходы CNN, для того чтобы убрать некоторые фичи из аудио. Следовательно моделька начинает контекстуализировать (предсказывать что в пропусках должно стоять). Из-за этого модели приходится выкручиваться, и эта глубокая нейронная сетка формирует латентные высоко-размерные непрерывные отображения аудио, которые объединяют голосовую информацию и лингвистическую.

Также там юзается
Learning with Cluster Ensembles, ну про это уже не буду душнить