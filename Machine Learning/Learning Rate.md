### Strategies for [[Gradient Descent]]

![[AdaGrad (Adaptive Gradient)#Adaptive Learning Rate (AdaGrad)]]

![[RMSProp#Adaptive Learning Rate with Balancing Coefficient (RMSProp)]]

![[Adam Optimizer]]

