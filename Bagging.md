(or **B**ootstrap **Agg**regat**ing**))

$\mu(X)$ - learning method
$\tilde{X}$ - random sub-sample (generated by [[Bootstrap]])

$b_{1}(x) = \mu(\tilde{X}_{1})$
$b_{2}(x)=\mu(\tilde{X}_{2})$              $-$ base models
$\vdots$
$b_{N}(x)= \mu(\tilde{X}_{N})$ 

$$
a_{N}(x) = \frac{1}{N}\sum_{i=1}^{n} b_{n}(x) = \frac{1}{N}\sum_{i=1}^{n}\mu(\tilde{X}_{n}) - \text{bagging}
$$
### Important results
-  $bias(\underbrace{ a_{n} }_{ learning method })$ = $bias(b_{n}) \implies$ if base models are weak, resulting model will also be weak. $\implies$
$\implies$ $b_{n}(x)$ should be more complex
- $var(a_{N})=\frac{1}{N}var(b_{n})+ \frac{N(N-1)}{N^{2}} \cdot cov(b_{n}(x), b_{m}(x))$
Where $$cov(b_{n}(x), b_{m}(x)) = \mathbb{E}_{x,y}\mathbb{E}_{x}\left( \mu(\tilde{X}_{n})-\mathbb{E}\mu(\tilde{X}_{n}) \right) \cdot \left( \mu(\tilde{X}_{m})-\mathbb{E}\mu(\tilde{X}_{m}) \right)$$
$\implies$ The less our base models are correlated $\implies$ the better.
So let's invent [[Random Forest]] from that.

## [[Decision Tree]]
For trees, using bagging
bias $\approx$ the same
variance $\downarrow$

